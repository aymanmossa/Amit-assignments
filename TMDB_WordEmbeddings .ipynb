{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pBsebGFzP6g"
      },
      "source": [
        "## Revisiting BOW, TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-060WfpdzIGY"
      },
      "source": [
        "#### **Part 1: Recap on BOW and TF-IDF**\n",
        "\n",
        "In this section, we will simply remind ourselves of the Bag of Words (BOW) and TF-IDF, as the focus will shift to word embeddings later.\n",
        "\n",
        "\n",
        "- **BOW** is a simple representation where each word in the document is represented by its frequency in the document.\n",
        "- It disregards grammar and word order but keeps track of word occurrences.\n",
        "\n",
        "```python\n",
        "# Example: Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"The dog barks\", \"The dog runs fast\", \"The cat sleeps\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Matrix:\\n\", X.toarray())\n",
        "```\n",
        "\n",
        "#### 1.2 **TF-IDF**\n",
        "- **TF-IDF** stands for \"Term Frequency-Inverse Document Frequency\".\n",
        "- It weighs words by how important they are in a document, considering the term frequency (TF) and how rare a term is in the entire corpus (IDF).\n",
        "\n",
        "```python\n",
        "# Example: TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmOLIuWlz03k"
      },
      "source": [
        "## Word-Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VN5Yq4Qz3Cj"
      },
      "source": [
        "#### 2.1 **What are Word Embeddings?**\n",
        "- Word embeddings are dense vector representations of words where similar words have similar vector representations.\n",
        "- Unlike traditional methods like **BOW** or **TF-IDF**, word embeddings capture semantic meaning and relationships between words, such as synonyms, antonyms, and even analogies.\n",
        "- Embeddings are learned from context, not just word frequency or importance.\n",
        "\n",
        "#### 2.2 **Why Are Word Embeddings Important?**\n",
        "- Word embeddings address the limitation of BOW and TF-IDF where word order and meaning are lost.\n",
        "- They allow models to understand semantic relationships between words. For instance, the vectors for **\"king\"** and **\"queen\"** would be closer than \"king\" and \"dog\".\n",
        "\n",
        "#### 2.3 **Types of Word Embeddings**\n",
        "Some of the famous word embedding algorithms are:\n",
        "- **Word2Vec**: Learns embeddings based on the context words of a target word (Skip-Gram and CBOW).\n",
        "- **GloVe**: Uses matrix factorization techniques to find word embeddings based on word co-occurrence.\n",
        "- **FastText**: An extension of Word2Vec that represents words as bags of character n-grams.\n",
        "\n",
        "We will focus on **Word2Vec**, a widely used algorithm, in the next steps.\n",
        "\n",
        "#### 2.4 **Skip-Gram and CBOW (Continuous Bag of Words)**\n",
        "\n",
        "- **Skip-Gram**: Predicts the surrounding context words (context) given a target word.\n",
        "  - E.g., Given \"dog\" as the target, predict words like \"barks\", \"chases\", etc.\n",
        "  \n",
        "- **CBOW**: Predicts the target word based on the surrounding context.\n",
        "  - E.g., Given words like \"barks\", \"chases\", predict the target word \"dog\".\n",
        "\n",
        "In the next part, we will start building a **Word2Vec model** using a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s1J5K0K0Adf"
      },
      "source": [
        "### **Building a Word2Vec Model using CBOW**\n",
        "\n",
        "#### 3.1 **Overview of CBOW**\n",
        "In the **CBOW** (Continuous Bag of Words) model, we are tasked with predicting the target word based on a given context. The context consists of a fixed-size window of surrounding words.\n",
        "\n",
        "For example:\n",
        "- Given the context words: \"the\", \"dog\", \"chases\"\n",
        "- The model tries to predict the target word: \"cat\"\n",
        "\n",
        "#### 3.2 **Steps to Build the CBOW Model**\n",
        "We will implement CBOW from scratch using a neural network with the following steps:\n",
        "1. **Data Preparation**: Convert the text into pairs of context-target words.\n",
        "2. **Create Vocabulary**: Map words to unique integers (word index).\n",
        "3. **Input Layer**: The context words will be one-hot encoded.\n",
        "4. **Hidden Layer**: The embeddings will be learned in this layer.\n",
        "5. **Output Layer**: Use softmax to predict the target word.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.3 **Implementing CBOW in Code**\n",
        "\n",
        "We'll need to install some libraries and use a sample corpus to build this model.\n",
        "\n",
        "```python\n",
        "# Install the necessary libraries\n",
        "!pip install numpy tensorflow\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"the dog barks\",\n",
        "    \"the dog chases the cat\",\n",
        "    \"the cat sleeps\",\n",
        "    \"the dog runs fast\"\n",
        "]\n",
        "\n",
        "# 1. Data Preparation: Create a context-target pair (CBOW)\n",
        "window_size = 2  # Context size is 2, meaning 2 words before and 2 after the target\n",
        "\n",
        "# Tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding (if needed)\n",
        "\n",
        "# Convert words to integer tokens\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "# Generate context-target pairs (X is context, y is target)\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        context = seq[i-window_size:i] + seq[i+1:i+window_size+1]  # Context words\n",
        "        target = seq[i]  # Target word\n",
        "        X.append(context)\n",
        "        y.append(target)\n",
        "\n",
        "# Pad the context words to ensure uniform length\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# 2. Create the CBOW Model\n",
        "embedding_dim = 50  # Size of the embedding vector\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=window_size * 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the Model\n",
        "model.summary()\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Now the embeddings are learned!\n",
        "```\n",
        "\n",
        "#### 3.4 **What Happens Here:**\n",
        "1. **Tokenizer**: We convert the words into sequences of integers (a word index).\n",
        "2. **Context-Target Generation**: For each word in a sequence, we generate a context of surrounding words, and the target is the central word.\n",
        "3. **Model Architecture**:\n",
        "   - **Embedding Layer**: This layer learns the word embeddings. The input size is the vocabulary size, and the output size is the embedding dimension (50 in this case).\n",
        "   - **Flatten Layer**: To make the output suitable for a Dense layer.\n",
        "   - **Dense Layer**: This gives the output probabilities for each word in the vocabulary.\n",
        "4. **Training**: The model is trained using the context-target pairs.\n",
        "\n",
        "The embeddings will be learned during the training process. After training, you can extract the word embeddings from the **embedding layer** of the model.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHZPTiu11C7x"
      },
      "source": [
        "## Skip-Gram, N-Skip-Gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfvV1r5s1Jfa"
      },
      "source": [
        "#### 4.1 **Overview of Skip-Gram**\n",
        "In the **Skip-Gram** model, the goal is to predict the context words given a target word. The target word is the central word in the context window, and the model learns to predict the surrounding words.\n",
        "\n",
        "For example:\n",
        "- Given the target word: \"dog\"\n",
        "- The model tries to predict words like \"barks\", \"chases\", etc.\n",
        "\n",
        "This is the reverse of the **CBOW** approach, where context predicts the target.\n",
        "\n",
        "#### 4.2 **Skip-Gram Implementation**\n",
        "\n",
        "We'll start by implementing the Skip-Gram model, similar to the previous CBOW approach, but the difference is that the target is the central word and the context words are predicted.\n",
        "\n",
        "```python\n",
        "# Skip-Gram model implementation\n",
        "\n",
        "# Context size (window size)\n",
        "window_size = 2\n",
        "\n",
        "# Generate context-target pairs for Skip-Gram\n",
        "X_skipgram = []\n",
        "y_skipgram = []\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        target = seq[i]  # Target word\n",
        "        context = seq[i-window_size:i] + seq[i+1:i+window_size+1]  # Context words\n",
        "        for word in context:\n",
        "            X_skipgram.append([target])\n",
        "            y_skipgram.append(word)\n",
        "\n",
        "X_skipgram = np.array(X_skipgram)\n",
        "y_skipgram = np.array(y_skipgram)\n",
        "\n",
        "# Build Skip-Gram model\n",
        "model_skipgram = Sequential()\n",
        "model_skipgram.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
        "model_skipgram.add(Flatten())\n",
        "model_skipgram.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model_skipgram.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the Skip-Gram model\n",
        "model_skipgram.summary()\n",
        "model_skipgram.fit(X_skipgram, y_skipgram, epochs=100, verbose=1)\n",
        "```\n",
        "\n",
        "#### 4.3 **What Happens in Skip-Gram:**\n",
        "- **Input**: The input is a single target word (central word) in the context window.\n",
        "- **Output**: The output is the probability distribution of context words. The model tries to maximize the probability of predicting the context words given the target word.\n",
        "\n",
        "The architecture here is quite similar to the CBOW, with the main difference being that Skip-Gram predicts multiple context words for a single target word.\n",
        "\n",
        "---\n",
        "\n",
        "### **Part 5: N-Skip-Gram Model**\n",
        "\n",
        "The **N-Skip-Gram** model is an extension of Skip-Gram, where the target word is used to predict the **N** context words, but instead of predicting each context word individually, the model tries to predict a larger set of context words (possibly with larger context windows).\n",
        "\n",
        "#### 5.1 **Difference with Skip-Gram**:\n",
        "- **Skip-Gram**: Predicts a set of context words for each target word. The model output is a probability distribution over the vocabulary for each context word.\n",
        "- **N-Skip-Gram**: Instead of predicting a fixed number of context words (as in Skip-Gram), we predict **N** context words over a larger window of words around the target word.\n",
        "\n",
        "Here’s how we can implement the **N-Skip-Gram** model:\n",
        "\n",
        "```python\n",
        "# N-Skip-Gram implementation\n",
        "N = 3  # We can vary N to predict more context words\n",
        "\n",
        "X_nskipgram = []\n",
        "y_nskipgram = []\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        target = seq[i]\n",
        "        context = seq[i-N:i] + seq[i+1:i+N+1]  # Larger context window (N words)\n",
        "        for word in context:\n",
        "            X_nskipgram.append([target])\n",
        "            y_nskipgram.append(word)\n",
        "\n",
        "X_nskipgram = np.array(X_nskipgram)\n",
        "y_nskipgram = np.array(y_nskipgram)\n",
        "\n",
        "# Build N-Skip-Gram model\n",
        "model_nskipgram = Sequential()\n",
        "model_nskipgram.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
        "model_nskipgram.add(Flatten())\n",
        "model_nskipgram.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model_nskipgram.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the N-Skip-Gram model\n",
        "model_nskipgram.summary()\n",
        "model_nskipgram.fit(X_nskipgram, y_nskipgram, epochs=100, verbose=1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Part 6: Difference Between Skip-Gram and N-Skip-Gram**\n",
        "\n",
        "- **Skip-Gram**: For each target word, predicts a fixed number of context words (one context word at a time). The size of the context window is usually fixed.\n",
        "- **N-Skip-Gram**: Expands on the Skip-Gram model by predicting multiple context words (based on a larger context window). The difference is that N-Skip-Gram tries to capture a broader context for each target word by considering more context words in its predictions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbeWscC_1Vql"
      },
      "source": [
        "## Pre-Trained Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlKNiZlR1X72"
      },
      "source": [
        "### **Part 5: Pre-trained Word Embeddings**\n",
        "\n",
        "#### 7.1 **What Are Pre-trained Word Embeddings?**\n",
        "\n",
        "Pre-trained word embeddings are vector representations of words that have been learned from large corpora of text and can be used directly in downstream tasks like text classification, sentiment analysis, etc.\n",
        "\n",
        "The benefit of pre-trained embeddings is that they capture rich semantic relationships between words, which can significantly improve performance in many NLP tasks compared to training embeddings from scratch.\n",
        "\n",
        "#### 7.2 **Types of Pre-trained Word Embeddings**\n",
        "\n",
        "1. **Word2Vec (Skip-Gram and CBOW)**:\n",
        "   - Word2Vec is an unsupervised learning algorithm used to learn word embeddings from a large corpus.\n",
        "   - The two models in Word2Vec are **Skip-Gram** and **CBOW** (which we discussed earlier).\n",
        "\n",
        "   The **Skip-Gram model** maximizes the following objective function:\n",
        "   \\[\n",
        "   J(\\theta) = -\\sum_{t=1}^{T} \\sum_{-C \\leq j \\leq C, j \\neq 0} \\log p(w_{t+j} | w_t)\n",
        "   \\]\n",
        "   - **Target Word**: \\(w_t\\)\n",
        "   - **Context Words**: \\(w_{t+j}\\) (within the context window \\(C\\))\n",
        "   - **Probability**: \\(p(w_{t+j} | w_t)\\) is the conditional probability of observing the context word given the target word.\n",
        "   \n",
        "   For the **CBOW** model, the goal is to predict the target word based on the context words. The equation is:\n",
        "   \\[\n",
        "   J(\\theta) = -\\sum_{t=1}^{T} \\log p(w_t | context_{t})\n",
        "   \\]\n",
        "   where the context is the surrounding words and the target is the center word.\n",
        "\n",
        "   - **Objective**: The models attempt to maximize the likelihood of the context given the target (Skip-Gram) or maximize the likelihood of the target given the context (CBOW).\n",
        "\n",
        "---\n",
        "\n",
        "2. **GloVe (Global Vectors for Word Representation)**:\n",
        "   - GloVe is a word embedding model based on **matrix factorization**. It uses the global word co-occurrence statistics of a corpus.\n",
        "   - The objective of GloVe is to factorize the word co-occurrence matrix \\(X\\) (where each element \\(X_{ij}\\) represents the number of times word \\(i\\) appears in the context of word \\(j\\)).\n",
        "   \n",
        "   The equation for GloVe is:\n",
        "   \\[\n",
        "   J(\\theta) = \\sum_{i=1}^{V} \\sum_{j=1}^{V} f(X_{ij}) \\left( w_i^T w_j + b_i + b_j - \\log X_{ij} \\right)^2\n",
        "   \\]\n",
        "   - **Objective**: The objective is to minimize the squared error between the actual co-occurrence (\\(\\log X_{ij}\\)) and the predicted co-occurrence (\\(w_i^T w_j + b_i + b_j\\)), with \\(f(X_{ij})\\) as a weighting function to balance the influence of rare and frequent co-occurrences.\n",
        "   - **\\(w_i\\)** and **\\(w_j\\)**: These are the embedding vectors for words \\(i\\) and \\(j\\).\n",
        "   - **\\(b_i\\)** and **\\(b_j\\)**: Bias terms for the words.\n",
        "\n",
        "   GloVe attempts to learn embeddings such that the dot product between two word vectors \\(w_i\\) and \\(w_j\\) is close to the log of the co-occurrence count.\n",
        "\n",
        "---\n",
        "\n",
        "3. **FastText**:\n",
        "   - FastText is an extension of Word2Vec where each word is represented as a bag of character n-grams. This allows the model to generate embeddings for **out-of-vocabulary words** (words not seen during training).\n",
        "   \n",
        "   The main difference with Word2Vec is that it uses subword information, so for a word \\(w\\), FastText breaks it into subwords (e.g., n-grams of characters).\n",
        "   - For example, the word “apple” can be represented as a combination of n-grams like **‘ap’, ‘pp’, ‘pl’, ‘le’** (depending on the n-gram size chosen).\n",
        "\n",
        "   The embedding for a word is a sum of the embeddings of all its subword n-grams.\n",
        "\n",
        "   The equation for FastText would be similar to Word2Vec but considering n-grams:\n",
        "   \\[\n",
        "   \\text{Emb}(w) = \\sum_{\\text{ngram}(w)} \\text{Emb}(\\text{ngram})\n",
        "   \\]\n",
        "   where the **ngram(w)** represents all character n-grams derived from the word \\(w\\).\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.3 **How to Use Pre-trained Word Embeddings**\n",
        "\n",
        "Using pre-trained embeddings involves loading an existing model that has been trained on a large corpus, such as **Google's Word2Vec**, **GloVe**, or **FastText**, and using these vectors for tasks like similarity measurement, text classification, etc.\n",
        "\n",
        "Here’s an example of how to load pre-trained **GloVe** embeddings and use them with a Keras model:\n",
        "\n",
        "```python\n",
        "# Load GloVe pre-trained embeddings\n",
        "import numpy as np\n",
        "\n",
        "# Define the embedding dimension and file path for GloVe (e.g., 100-dimensional vectors)\n",
        "embedding_dim = 100\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "# Load the GloVe word vectors into a dictionary\n",
        "embeddings_index = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
        "\n",
        "# Example: Access the embedding for the word 'king'\n",
        "embedding_king = embeddings_index['king']\n",
        "print(\"Embedding for 'king':\", embedding_king)\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbWye5uwJ-Wm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nJKzGzsJ-TG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdKltPFnJ-RB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VwpsN00L_jM"
      },
      "source": [
        "# TMDb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvnNhUuLzOpn"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import html\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOz1LhpcJIId"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/sample_data/top10K-TMDB-movies.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "vvYWW1aYJPbq",
        "outputId": "20d61ff1-ee32-43df-d383-7178eda0da19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                          title  \\\n",
              "0        278                       The Shawshank Redemption   \n",
              "1      19404                    Dilwale Dulhania Le Jayenge   \n",
              "2        238                                  The Godfather   \n",
              "3        424                               Schindler's List   \n",
              "4        240                         The Godfather: Part II   \n",
              "...      ...                                            ...   \n",
              "9995   10196                             The Last Airbender   \n",
              "9996  331446                       Sharknado 3: Oh Hell No!   \n",
              "9997   13995                                Captain America   \n",
              "9998    2312  In the Name of the King: A Dungeon Siege Tale   \n",
              "9999  455957                                         Domino   \n",
              "\n",
              "                                                 genre original_language  \\\n",
              "0                                          Drama,Crime                en   \n",
              "1                                 Comedy,Drama,Romance                hi   \n",
              "2                                          Drama,Crime                en   \n",
              "3                                    Drama,History,War                en   \n",
              "4                                          Drama,Crime                en   \n",
              "...                                                ...               ...   \n",
              "9995                          Action,Adventure,Fantasy                en   \n",
              "9996  Action,TV Movie,Science Fiction,Comedy,Adventure                en   \n",
              "9997                        Action,Science Fiction,War                en   \n",
              "9998                    Adventure,Fantasy,Action,Drama                en   \n",
              "9999                             Thriller,Action,Crime                en   \n",
              "\n",
              "                                               overview  popularity  \\\n",
              "0     Framed in the 1940s for the double murder of h...      94.075   \n",
              "1     Raj is a rich, carefree, happy-go-lucky second...      25.408   \n",
              "2     Spanning the years 1945 to 1955, a chronicle o...      90.585   \n",
              "3     The true story of how businessman Oskar Schind...      44.761   \n",
              "4     In the continuing saga of the Corleone crime f...      57.749   \n",
              "...                                                 ...         ...   \n",
              "9995  The story follows the adventures of Aang, a yo...      98.322   \n",
              "9996  The sharks take bite out of the East Coast whe...      12.490   \n",
              "9997  During World War II, a brave, patriotic Americ...      18.333   \n",
              "9998  A man named Farmer sets out to rescue his kidn...      15.159   \n",
              "9999  Seeking justice for his partner’s murder by an...      16.482   \n",
              "\n",
              "     release_date  vote_average  vote_count  \n",
              "0      1994-09-23           8.7       21862  \n",
              "1      1995-10-19           8.7        3731  \n",
              "2      1972-03-14           8.7       16280  \n",
              "3      1993-12-15           8.6       12959  \n",
              "4      1974-12-20           8.6        9811  \n",
              "...           ...           ...         ...  \n",
              "9995   2010-06-30           4.7        3347  \n",
              "9996   2015-07-22           4.7         417  \n",
              "9997   1990-12-14           4.6         332  \n",
              "9998   2007-11-29           4.7         668  \n",
              "9999   2019-05-31           4.6         221  \n",
              "\n",
              "[10000 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9282da17-ffab-4ff8-b252-fcb240521483\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>genre</th>\n",
              "      <th>original_language</th>\n",
              "      <th>overview</th>\n",
              "      <th>popularity</th>\n",
              "      <th>release_date</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>vote_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>278</td>\n",
              "      <td>The Shawshank Redemption</td>\n",
              "      <td>Drama,Crime</td>\n",
              "      <td>en</td>\n",
              "      <td>Framed in the 1940s for the double murder of h...</td>\n",
              "      <td>94.075</td>\n",
              "      <td>1994-09-23</td>\n",
              "      <td>8.7</td>\n",
              "      <td>21862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19404</td>\n",
              "      <td>Dilwale Dulhania Le Jayenge</td>\n",
              "      <td>Comedy,Drama,Romance</td>\n",
              "      <td>hi</td>\n",
              "      <td>Raj is a rich, carefree, happy-go-lucky second...</td>\n",
              "      <td>25.408</td>\n",
              "      <td>1995-10-19</td>\n",
              "      <td>8.7</td>\n",
              "      <td>3731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>238</td>\n",
              "      <td>The Godfather</td>\n",
              "      <td>Drama,Crime</td>\n",
              "      <td>en</td>\n",
              "      <td>Spanning the years 1945 to 1955, a chronicle o...</td>\n",
              "      <td>90.585</td>\n",
              "      <td>1972-03-14</td>\n",
              "      <td>8.7</td>\n",
              "      <td>16280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>424</td>\n",
              "      <td>Schindler's List</td>\n",
              "      <td>Drama,History,War</td>\n",
              "      <td>en</td>\n",
              "      <td>The true story of how businessman Oskar Schind...</td>\n",
              "      <td>44.761</td>\n",
              "      <td>1993-12-15</td>\n",
              "      <td>8.6</td>\n",
              "      <td>12959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>240</td>\n",
              "      <td>The Godfather: Part II</td>\n",
              "      <td>Drama,Crime</td>\n",
              "      <td>en</td>\n",
              "      <td>In the continuing saga of the Corleone crime f...</td>\n",
              "      <td>57.749</td>\n",
              "      <td>1974-12-20</td>\n",
              "      <td>8.6</td>\n",
              "      <td>9811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>10196</td>\n",
              "      <td>The Last Airbender</td>\n",
              "      <td>Action,Adventure,Fantasy</td>\n",
              "      <td>en</td>\n",
              "      <td>The story follows the adventures of Aang, a yo...</td>\n",
              "      <td>98.322</td>\n",
              "      <td>2010-06-30</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>331446</td>\n",
              "      <td>Sharknado 3: Oh Hell No!</td>\n",
              "      <td>Action,TV Movie,Science Fiction,Comedy,Adventure</td>\n",
              "      <td>en</td>\n",
              "      <td>The sharks take bite out of the East Coast whe...</td>\n",
              "      <td>12.490</td>\n",
              "      <td>2015-07-22</td>\n",
              "      <td>4.7</td>\n",
              "      <td>417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>13995</td>\n",
              "      <td>Captain America</td>\n",
              "      <td>Action,Science Fiction,War</td>\n",
              "      <td>en</td>\n",
              "      <td>During World War II, a brave, patriotic Americ...</td>\n",
              "      <td>18.333</td>\n",
              "      <td>1990-12-14</td>\n",
              "      <td>4.6</td>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>2312</td>\n",
              "      <td>In the Name of the King: A Dungeon Siege Tale</td>\n",
              "      <td>Adventure,Fantasy,Action,Drama</td>\n",
              "      <td>en</td>\n",
              "      <td>A man named Farmer sets out to rescue his kidn...</td>\n",
              "      <td>15.159</td>\n",
              "      <td>2007-11-29</td>\n",
              "      <td>4.7</td>\n",
              "      <td>668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>455957</td>\n",
              "      <td>Domino</td>\n",
              "      <td>Thriller,Action,Crime</td>\n",
              "      <td>en</td>\n",
              "      <td>Seeking justice for his partner’s murder by an...</td>\n",
              "      <td>16.482</td>\n",
              "      <td>2019-05-31</td>\n",
              "      <td>4.6</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9282da17-ffab-4ff8-b252-fcb240521483')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9282da17-ffab-4ff8-b252-fcb240521483 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9282da17-ffab-4ff8-b252-fcb240521483');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e05a0b42-8f8c-414f-b3d7-5e68b4a1cbd5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e05a0b42-8f8c-414f-b3d7-5e68b4a1cbd5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e05a0b42-8f8c-414f-b3d7-5e68b4a1cbd5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9b1e9afe-ac2e-4eaa-b030-8587187f77cd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9b1e9afe-ac2e-4eaa-b030-8587187f77cd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 211422,\n        \"min\": 5,\n        \"max\": 934761,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          332567,\n          727293,\n          9385\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9661,\n        \"samples\": [\n          \"9 Month Stretch\",\n          \"Detention\",\n          \"Welcome\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2123,\n        \"samples\": [\n          \"Adventure,Comedy,Drama,Romance\",\n          \"Comedy,Mystery,Crime,Adventure,Action\",\n          \"Action,Adventure,Comedy,Crime,Drama,Thriller\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 43,\n        \"samples\": [\n          \"uk\",\n          \"et\",\n          \"id\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overview\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9985,\n        \"samples\": [\n          \"An obsessed cop is on the trail of a serial killer prowling the streets of Buffalo, N.Y. but when his teenage daughter disappears, he drops any professional restraint to get the killer.\",\n          \"After fighting his demons for decades, John Rambo now lives in peace on his family ranch in Arizona, but his rest is interrupted when Gabriela, the granddaughter of his housekeeper Mar\\u00eda, disappears after crossing the border into Mexico to meet her biological father. Rambo, who has become a true father figure for Gabriela over the years, undertakes a desperate and dangerous journey to find her.\",\n          \"Hubie Dubois, despite his devotion to his hometown of Salem, Massachusetts (and its legendary Halloween celebration), is a figure of mockery for kids and adults alike. But this year, something really is going bump in the night, and it\\u2019s up to Hubie to save Halloween.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"popularity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 211.68417479335739,\n        \"min\": 0.6,\n        \"max\": 10436.917,\n        \"num_unique_values\": 8511,\n        \"samples\": [\n          19.394,\n          134.7,\n          58.511\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"release_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 6113,\n        \"samples\": [\n          \"2009-02-24\",\n          \"1980-11-14\",\n          \"1979-05-02\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vote_average\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7662312891933013,\n        \"min\": 4.6,\n        \"max\": 8.7,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          6.2,\n          7.4,\n          7.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vote_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2648,\n        \"min\": 200,\n        \"max\": 31917,\n        \"num_unique_values\": 3191,\n        \"samples\": [\n          10895,\n          984,\n          12415\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEMNbgdtJbRf"
      },
      "outputs": [],
      "source": [
        "text_row = df['overview']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qKeY0PMtJf1w",
        "outputId": "10e74fbb-f12c-47a6-cd06-db3f169061ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In the continuing saga of the Corleone crime family, a young Vito Corleone grows up in Sicily and in 1910s New York. In the 1950s, Michael Corleone attempts to expand the family business into Las Vegas, Hollywood and Cuba.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text_row[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnkhh-gwJ4CQ"
      },
      "source": [
        "## Preprocessing using spaCy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7H7HZieJ27L"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MCqxJMrOEck"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Check if text is a string, if not convert it\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        doc = nlp(text.lower())\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove HTML entities\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    tokens = [token.lemma_\n",
        "              for token in doc\n",
        "              if not token.is_stop\n",
        "              and not token.is_punct\n",
        "              and not token.is_space\n",
        "    ]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NqVL_LsLX-x"
      },
      "outputs": [],
      "source": [
        "text_pre = text_row.apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "EPG10Z3MUmWa",
        "outputId": "4039c4e5-4200-4206-f66b-4e2e9a7205df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       frame s double murder wife lover upstande bank...\n",
              "1       raj rich carefree happy lucky second generatio...\n",
              "2       span year chronicle fictional italian american...\n",
              "3       true story businessman oskar schindler save th...\n",
              "4       continue saga corleone crime family young vito...\n",
              "                              ...                        \n",
              "9995    story follow adventure aang young successor lo...\n",
              "9996    shark bite east coast sharknado hit washington...\n",
              "9997    world war ii brave patriotic american soldier ...\n",
              "9998    man name farmer set rescue kidnap wife avenge ...\n",
              "9999    seek justice partner murder isis member copenh...\n",
              "Name: overview, Length: 10000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>frame s double murder wife lover upstande bank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>raj rich carefree happy lucky second generatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>span year chronicle fictional italian american...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>true story businessman oskar schindler save th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>continue saga corleone crime family young vito...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>story follow adventure aang young successor lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>shark bite east coast sharknado hit washington...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>world war ii brave patriotic american soldier ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>man name farmer set rescue kidnap wife avenge ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>seek justice partner murder isis member copenh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "text_pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXUvLN3KUO1F"
      },
      "source": [
        "## Tokenze preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lypQq9gmLfYz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ritQVlPTUb9S"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_pre)\n",
        "sequence = tokenizer.texts_to_sequences(text_pre)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLTK5jbf6PEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Skip-Gram Model"
      ],
      "metadata": {
        "id": "RgwkJLyv6xgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9nT9dct52nT",
        "outputId": "ac77891e-6c88-46f6-c0a4-a085e9d915a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 22556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "N = 3\n",
        "window_size = 2\n",
        "embedding_dim = 50\n",
        "epochs = 5\n",
        "\n",
        "\n",
        "# N-Skip-Gram Matrix\n",
        "X_nskipgram = []\n",
        "y_nskipgram = []\n",
        "\n",
        "for seq in sequence:  # Ensure sequences is correctly populated\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        target = seq[i]\n",
        "        context = seq[max(0, i - N):i] + seq[i + 1:min(len(seq), i + N + 1)]\n",
        "        for word in context:\n",
        "            X_nskipgram.append([target])\n",
        "            y_nskipgram.append(word)\n",
        "\n",
        "X_nskipgram = np.array(X_nskipgram)\n",
        "y_nskipgram = np.array(y_nskipgram)"
      ],
      "metadata": {
        "id": "kyb3HVjZ6Ko2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "ql7SwwLYU7sU",
        "outputId": "855e9b75-0b62-49b0-d1cd-76bdbcbd8c94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │       \u001b[38;5;34m1,127,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22556\u001b[0m)               │       \u001b[38;5;34m1,150,356\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,127,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22556</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,150,356</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,278,156\u001b[0m (8.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,278,156</span> (8.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,278,156\u001b[0m (8.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,278,156</span> (8.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Build N-Skip-Gram model\n",
        "model_nskipgram = Sequential()\n",
        "model_nskipgram.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
        "model_nskipgram.add(Flatten())\n",
        "model_nskipgram.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Explicitly build the model\n",
        "model_nskipgram.build(input_shape=(None, 1))\n",
        "\n",
        "# Compile Model\n",
        "model_nskipgram.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Model Summary\n",
        "model_nskipgram.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the N-Skip-Gram model\n",
        "model_nskipgram.fit(X_nskipgram, y_nskipgram, epochs=epochs, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh4Yq9ID6WgC",
        "outputId": "836d53c7-cba6-4b74-ffb0-688ded639113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m38649/38649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2ms/step - accuracy: 0.0090 - loss: 8.7036\n",
            "Epoch 2/5\n",
            "\u001b[1m38649/38649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2ms/step - accuracy: 0.0162 - loss: 8.2820\n",
            "Epoch 3/5\n",
            "\u001b[1m38649/38649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2ms/step - accuracy: 0.0205 - loss: 8.0656\n",
            "Epoch 4/5\n",
            "\u001b[1m38649/38649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2ms/step - accuracy: 0.0223 - loss: 7.9190\n",
            "Epoch 5/5\n",
            "\u001b[1m38649/38649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2ms/step - accuracy: 0.0230 - loss: 7.8045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b952210da20>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJQ9gCBG942E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word2vec model using cbow"
      ],
      "metadata": {
        "id": "T8VHdokEFu-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cbow matrices\n",
        "x_cbow = []\n",
        "y_cbow = []\n",
        "\n",
        "for seq in sequence:\n",
        "    for i in range(window_size, len(seq) - window_size):\n",
        "        context = seq[i-window_size:i] + seq[i+1:i+window_size+1]\n",
        "        target = seq[i]\n",
        "        x_cbow.append(context)\n",
        "        y_cbow.append(target)\n",
        "\n",
        "x_cbow = np.array(x_cbow)\n",
        "y_cbow = np.array(y_cbow)\n",
        "\n"
      ],
      "metadata": {
        "id": "S20eEDqSFMCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging Data Preparation\n",
        "print(\"X shape:\", x_cbow.shape)\n",
        "print(\"y_cbow shape:\", y_cbow.shape)\n",
        "print(\"Sample X_cbow:\", x_cbow[:5])\n",
        "print(\"Sample y_cbow:\", y_cbow[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIcA9daNNju6",
        "outputId": "c6be1225-fca9-4979-d7b4-e1f4589ed37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (209455, 4)\n",
            "y_cbow shape: (209455,)\n",
            "Sample X_cbow: [[1142   62   48   36]\n",
            " [  62 1033   36  307]\n",
            " [1033   48  307 7753]\n",
            " [  48   36 7753 2993]\n",
            " [  36  307 2993 1405]]\n",
            "Sample y_cbow: [1033   48   36  307 7753]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CBoW Model\n",
        "\n",
        "model_cbow = Sequential()\n",
        "model_cbow.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=window_size * 2))\n",
        "model_cbow.add(Flatten())\n",
        "model_cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Explicitly build the model\n",
        "model_cbow.build(input_shape=(None, window_size * 2))\n",
        "\n",
        "model_cbow.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_cbow.summary()\n",
        "\n",
        "(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "u1wLWTBpIX1e",
        "outputId": "ca378939-4983-4205-e7ac-099bed7bc4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │       \u001b[38;5;34m1,127,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22556\u001b[0m)               │       \u001b[38;5;34m4,533,756\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,127,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22556</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,533,756</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,661,556\u001b[0m (21.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,661,556</span> (21.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,661,556\u001b[0m (21.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,661,556</span> (21.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Embedding name=embedding_14, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train th CBoW model\n",
        "model_cbow.fit(x_cbow, y_cbow, epochs=epochs, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2RDMLGhVZOW",
        "outputId": "4cab9adb-3cc4-4db6-887e-df4092c28050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m6546/6546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.0160 - loss: 8.9072\n",
            "Epoch 2/5\n",
            "\u001b[1m6546/6546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.0480 - loss: 7.8327\n",
            "Epoch 3/5\n",
            "\u001b[1m6546/6546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.0833 - loss: 6.8780\n",
            "Epoch 4/5\n",
            "\u001b[1m6546/6546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.1304 - loss: 5.8976\n",
            "Epoch 5/5\n",
            "\u001b[1m6546/6546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.1970 - loss: 5.0337\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b93a762b340>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iA0tfNNEVhBY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}